{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/david.amat/Documents/david/pdf-search-llm-rag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Get the absolute path of the current project directory\n",
    "project_dir = os.path.abspath('.')\n",
    "\n",
    "# Get the parent of the parent directory\n",
    "WORK_DIR = os.path.abspath(os.path.join(project_dir, '../../'))\n",
    "\n",
    "# Change the working directory to the parent of the parent directory\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "# Verify the change by printing the current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process_pdf_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_title = \"attention_is_all_you_need\"\n",
    "pdf_path = f\"data/{file_title}.pdf\"\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of pages: 15\n"
     ]
    }
   ],
   "source": [
    "pages_amount = len(pdf_reader.pages)\n",
    "print(f'Amount of pages: {pages_amount}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, page in enumerate(pdf_reader.pages):\n",
    "    document_text.append([file_title, i+1, page.extract_text()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(document_text[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"file_name\", \"page_number\", \"text\"]\n",
    "df_text = pd.DataFrame(document_text, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>page_number</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>Provided proper attribution is provided, Googl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>2</td>\n",
       "      <td>1 Introduction\\nRecurrent neural networks, lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>3</td>\n",
       "      <td>Figure 1: The Transformer - model architecture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>4</td>\n",
       "      <td>Scaled Dot-Product Attention\\n Multi-Head Atte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>5</td>\n",
       "      <td>output values. These are concatenated and once...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>6</td>\n",
       "      <td>Table 1: Maximum path lengths, per-layer compl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>7</td>\n",
       "      <td>length nis smaller than the representation dim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>8</td>\n",
       "      <td>Table 2: The Transformer achieves better BLEU ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>9</td>\n",
       "      <td>Table 3: Variations on the Transformer archite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>10</td>\n",
       "      <td>Table 4: The Transformer generalizes well to E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>11</td>\n",
       "      <td>[5]Kyunghyun Cho, Bart van Merrienboer, Caglar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>12</td>\n",
       "      <td>[25] Mitchell P Marcus, Mary Ann Marcinkiewicz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>13</td>\n",
       "      <td>Attention Visualizations\\nInput-Input Layer5\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>14</td>\n",
       "      <td>Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file_name  page_number  \\\n",
       "0   attention_is_all_you_need            1   \n",
       "1   attention_is_all_you_need            2   \n",
       "2   attention_is_all_you_need            3   \n",
       "3   attention_is_all_you_need            4   \n",
       "4   attention_is_all_you_need            5   \n",
       "5   attention_is_all_you_need            6   \n",
       "6   attention_is_all_you_need            7   \n",
       "7   attention_is_all_you_need            8   \n",
       "8   attention_is_all_you_need            9   \n",
       "9   attention_is_all_you_need           10   \n",
       "10  attention_is_all_you_need           11   \n",
       "11  attention_is_all_you_need           12   \n",
       "12  attention_is_all_you_need           13   \n",
       "13  attention_is_all_you_need           14   \n",
       "14  attention_is_all_you_need           15   \n",
       "\n",
       "                                                 text  \n",
       "0   Provided proper attribution is provided, Googl...  \n",
       "1   1 Introduction\\nRecurrent neural networks, lon...  \n",
       "2   Figure 1: The Transformer - model architecture...  \n",
       "3   Scaled Dot-Product Attention\\n Multi-Head Atte...  \n",
       "4   output values. These are concatenated and once...  \n",
       "5   Table 1: Maximum path lengths, per-layer compl...  \n",
       "6   length nis smaller than the representation dim...  \n",
       "7   Table 2: The Transformer achieves better BLEU ...  \n",
       "8   Table 3: Variations on the Transformer archite...  \n",
       "9   Table 4: The Transformer generalizes well to E...  \n",
       "10  [5]Kyunghyun Cho, Bart van Merrienboer, Caglar...  \n",
       "11  [25] Mitchell P Marcus, Mary Ann Marcinkiewicz...  \n",
       "12  Attention Visualizations\\nInput-Input Layer5\\n...  \n",
       "13  Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\...  \n",
       "14  Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_num = 1\n",
    "start_text = document_text[page_num][2][:25]\n",
    "end_text = document_text[page_num][2][-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_text[page_num][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 Introduction\\nRecurrent '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction\n",
      "Recurrent \n",
      "en generating the next.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(start_text)\n",
    "print(end_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_page(document: str) -> List[str]:\n",
    "    return document.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Explode the list to separate rows\n",
    "    data_text_exploded = data.explode('text')\n",
    "\n",
    "    # Add a 'paragraph' column indicating the index of the element in the list\n",
    "    data_text_exploded['paragraph'] = data_text_exploded.groupby(\n",
    "        ['file_name', 'page_number']\n",
    "    ).cumcount() + 1\n",
    "\n",
    "    return data_text_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply split_page function to split text into paragraphs\n",
    "df['text'] = df['text'].apply(split_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0: 50 paragraphs\n",
      "Page 1: 50 paragraphs\n",
      "Page 2: 23 paragraphs\n",
      "Page 3: 35 paragraphs\n",
      "Page 4: 49 paragraphs\n",
      "Page 5: 47 paragraphs\n",
      "Page 6: 45 paragraphs\n",
      "Page 7: 46 paragraphs\n",
      "Page 8: 51 paragraphs\n",
      "Page 9: 48 paragraphs\n",
      "Page 10: 45 paragraphs\n",
      "Page 11: 43 paragraphs\n",
      "Page 12: 73 paragraphs\n",
      "Page 13: 113 paragraphs\n",
      "Page 14: 113 paragraphs\n"
     ]
    }
   ],
   "source": [
    "for page_num in range(len(df['text'])):\n",
    "    num_paragraphs = len(df['text'][page_num])\n",
    "    print(f\"Page {page_num}: {num_paragraphs} paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text_exploded = df.explode('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>page_number</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>Provided proper attribution is provided, Googl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>reproduce the tables and figures in this paper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>scholarly works.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>Ashish Vaswani∗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>&lt;pad&gt;Figure 5: Many of the attention heads exh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>sentence. We give two such examples above, fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>at layer 5 of 6. The heads clearly learned to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>831 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file_name  page_number  \\\n",
       "0   attention_is_all_you_need            1   \n",
       "0   attention_is_all_you_need            1   \n",
       "0   attention_is_all_you_need            1   \n",
       "0   attention_is_all_you_need            1   \n",
       "0   attention_is_all_you_need            1   \n",
       "..                        ...          ...   \n",
       "14  attention_is_all_you_need           15   \n",
       "14  attention_is_all_you_need           15   \n",
       "14  attention_is_all_you_need           15   \n",
       "14  attention_is_all_you_need           15   \n",
       "14  attention_is_all_you_need           15   \n",
       "\n",
       "                                                 text  \n",
       "0   Provided proper attribution is provided, Googl...  \n",
       "0   reproduce the tables and figures in this paper...  \n",
       "0                                    scholarly works.  \n",
       "0                           Attention Is All You Need  \n",
       "0                                     Ashish Vaswani∗  \n",
       "..                                                ...  \n",
       "14                                              <EOS>  \n",
       "14  <pad>Figure 5: Many of the attention heads exh...  \n",
       "14  sentence. We give two such examples above, fro...  \n",
       "14  at layer 5 of 6. The heads clearly learned to ...  \n",
       "14                                                 15  \n",
       "\n",
       "[831 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text_exploded['paragraph'] = data_text_exploded.groupby(\n",
    "    ['file_name', 'page_number']\n",
    ").cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>page_number</th>\n",
       "      <th>text</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>Provided proper attribution is provided, Googl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>reproduce the tables and figures in this paper...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>scholarly works.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>Ashish Vaswani∗</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>&lt;EOS&gt;</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>&lt;pad&gt;Figure 5: Many of the attention heads exh...</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>sentence. We give two such examples above, fro...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>at layer 5 of 6. The heads clearly learned to ...</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>831 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file_name  page_number  \\\n",
       "0   attention_is_all_you_need            1   \n",
       "0   attention_is_all_you_need            1   \n",
       "0   attention_is_all_you_need            1   \n",
       "0   attention_is_all_you_need            1   \n",
       "0   attention_is_all_you_need            1   \n",
       "..                        ...          ...   \n",
       "14  attention_is_all_you_need           15   \n",
       "14  attention_is_all_you_need           15   \n",
       "14  attention_is_all_you_need           15   \n",
       "14  attention_is_all_you_need           15   \n",
       "14  attention_is_all_you_need           15   \n",
       "\n",
       "                                                 text  paragraph  \n",
       "0   Provided proper attribution is provided, Googl...          1  \n",
       "0   reproduce the tables and figures in this paper...          2  \n",
       "0                                    scholarly works.          3  \n",
       "0                           Attention Is All You Need          4  \n",
       "0                                     Ashish Vaswani∗          5  \n",
       "..                                                ...        ...  \n",
       "14                                              <EOS>        109  \n",
       "14  <pad>Figure 5: Many of the attention heads exh...        110  \n",
       "14  sentence. We give two such examples above, fro...        111  \n",
       "14  at layer 5 of 6. The heads clearly learned to ...        112  \n",
       "14                                                 15        113  \n",
       "\n",
       "[831 rows x 4 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply get_paragraphs function to explode the list and add paragraph numbers\n",
    "df = get_paragraphs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>page_number</th>\n",
       "      <th>text</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>Provided proper attribution is provided, Googl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>reproduce the tables and figures in this paper...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>scholarly works.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>Ashish Vaswani∗</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>&lt;EOS&gt;</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>&lt;pad&gt;Figure 5: Many of the attention heads exh...</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>sentence. We give two such examples above, fro...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>at layer 5 of 6. The heads clearly learned to ...</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>831 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file_name  page_number  \\\n",
       "0   attention_is_all_you_need            1   \n",
       "0   attention_is_all_you_need            1   \n",
       "0   attention_is_all_you_need            1   \n",
       "0   attention_is_all_you_need            1   \n",
       "0   attention_is_all_you_need            1   \n",
       "..                        ...          ...   \n",
       "14  attention_is_all_you_need           15   \n",
       "14  attention_is_all_you_need           15   \n",
       "14  attention_is_all_you_need           15   \n",
       "14  attention_is_all_you_need           15   \n",
       "14  attention_is_all_you_need           15   \n",
       "\n",
       "                                                 text  paragraph  \n",
       "0   Provided proper attribution is provided, Googl...          1  \n",
       "0   reproduce the tables and figures in this paper...          2  \n",
       "0                                    scholarly works.          3  \n",
       "0                           Attention Is All You Need          4  \n",
       "0                                     Ashish Vaswani∗          5  \n",
       "..                                                ...        ...  \n",
       "14                                              <EOS>        109  \n",
       "14  <pad>Figure 5: Many of the attention heads exh...        110  \n",
       "14  sentence. We give two such examples above, fro...        111  \n",
       "14  at layer 5 of 6. The heads clearly learned to ...        112  \n",
       "14                                                 15        113  \n",
       "\n",
       "[831 rows x 4 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply strip to remove leading and trailing spaces\n",
    "df['text'] = df['text'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['file_name', 'page_number', 'paragraph', 'text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Apply split_page function to split text into paragraphs\n",
    "    df['text'] = df['text'].apply(split_page)\n",
    "\n",
    "    # Apply get_paragraphs function to explode the list and add paragraph numbers\n",
    "    df = get_paragraphs(df)\n",
    "\n",
    "    # Apply strip to remove leading and trailing spaces\n",
    "    df['text'] = df['text'].str.strip()\n",
    "\n",
    "    # Reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Reorder columns for better readability\n",
    "    df = df[['file_name', 'page_number', 'paragraph', 'text']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_text.copy()\n",
    "df_filtered = process_text_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>page_number</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Provided proper attribution is provided, Googl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>reproduce the tables and figures in this paper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>scholarly works.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Ashish Vaswani∗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>109</td>\n",
       "      <td>&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>110</td>\n",
       "      <td>&lt;pad&gt;Figure 5: Many of the attention heads exh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>111</td>\n",
       "      <td>sentence. We give two such examples above, fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>112</td>\n",
       "      <td>at layer 5 of 6. The heads clearly learned to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>attention_is_all_you_need</td>\n",
       "      <td>15</td>\n",
       "      <td>113</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>831 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     file_name  page_number  paragraph  \\\n",
       "0    attention_is_all_you_need            1          1   \n",
       "1    attention_is_all_you_need            1          2   \n",
       "2    attention_is_all_you_need            1          3   \n",
       "3    attention_is_all_you_need            1          4   \n",
       "4    attention_is_all_you_need            1          5   \n",
       "..                         ...          ...        ...   \n",
       "826  attention_is_all_you_need           15        109   \n",
       "827  attention_is_all_you_need           15        110   \n",
       "828  attention_is_all_you_need           15        111   \n",
       "829  attention_is_all_you_need           15        112   \n",
       "830  attention_is_all_you_need           15        113   \n",
       "\n",
       "                                                  text  \n",
       "0    Provided proper attribution is provided, Googl...  \n",
       "1    reproduce the tables and figures in this paper...  \n",
       "2                                     scholarly works.  \n",
       "3                            Attention Is All You Need  \n",
       "4                                      Ashish Vaswani∗  \n",
       "..                                                 ...  \n",
       "826                                              <EOS>  \n",
       "827  <pad>Figure 5: Many of the attention heads exh...  \n",
       "828  sentence. We give two such examples above, fro...  \n",
       "829  at layer 5 of 6. The heads clearly learned to ...  \n",
       "830                                                 15  \n",
       "\n",
       "[831 rows x 4 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_parquet(\"data/debug_read_pdf.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr_llm_rag",
   "language": "python",
   "name": "kr_llm_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
