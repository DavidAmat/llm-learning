{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOURCE**: https://docs.llamaindex.ai/en/stable/examples/low_level/oss_ingestion_retrieval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/david.amat/Documents/david/pdf-search-llm-rag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Get the absolute path of the current project directory\n",
    "project_dir = os.path.abspath('.')\n",
    "\n",
    "# Get the parent of the parent directory\n",
    "WORK_DIR = os.path.abspath(os.path.join(project_dir, '../../'))\n",
    "\n",
    "# Change the working directory to the parent of the parent directory\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "# Verify the change by printing the current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/llama2.pdf: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFReader()\n",
    "file_title = \"llama2\"\n",
    "pdf_path = f\"data/{file_title}.pdf\"\n",
    "documents = loader.load(file_path=pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying sentence splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_parser = SentenceSplitter(\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=2,\n",
    "    paragraph_separator=\".\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Recurrent neural networks, long short-term memory',\n",
       " 'memory [13] and gated recurrent [7]',\n",
       " 'neural networks.\\n In particular we are interested in',\n",
       " 'interested in knowing what splits of the text this splitter',\n",
       " 'this splitter can provide']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_parser.split_text(\"Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks.\\n In particular we are interested in knowing what splits of the text this splitter can provide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split PDF document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    # separator=\" \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = []\n",
    "# maintain relationship with source doc index, to help inject doc metadata in (3)\n",
    "doc_idxs = []\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    cur_text_chunks = text_parser.split_text(doc.text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Construct Nodes from Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "    )\n",
    "    src_doc = documents[doc_idxs[idx]]\n",
    "    node.metadata = src_doc.metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings for each Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "MODEL_SENTENCE_TRANSFORMER = 'all-MiniLM-L6-v2'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer(\n",
    "    MODEL_SENTENCE_TRANSFORMER,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_pages': 77, 'file_path': 'data/llama2.pdf', 'source': '77'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:02<00:00, 41.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "for node in tqdm.tqdm(nodes):\n",
    "    node_embedding = model.encode(node.text)\n",
    "    node.embedding = list(node_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(node_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up PostgreSQL with: https://www.sqlshack.com/setting-up-a-postgresql-database-on-mac/\n",
    "\n",
    "```bash\n",
    "brew install postgresql\n",
    "brew services start postgresql\n",
    "# brew services stop postgresql\n",
    "\n",
    "# Configure psql\n",
    "psql postgres\n",
    "\n",
    "# <Inside PostgreSQL>\n",
    "CREATE ROLE david WITH LOGIN PASSWORD 'qrks';\n",
    "ALTER ROLE david WITH SUPERUSER;\n",
    "ALTER ROLE david CREATEDB;\n",
    "\n",
    "# New Logins\n",
    "psql postgres -U david\n",
    "\n",
    "# List users and roles\n",
    "\\du\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "db_name = \"vector_db\"\n",
    "host = \"localhost\"\n",
    "password = \"qrks\"\n",
    "port = \"5432\"\n",
    "user = \"david\"\n",
    "# conn = psycopg2.connect(connection_string)\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    ")\n",
    "conn.autocommit = True\n",
    "\n",
    "#with conn.cursor() as c:\n",
    "#    c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "#    c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# New Logins\n",
    "psql postgres -U david\n",
    "\n",
    "# in PostgreSQL CLI\n",
    "\\t\n",
    "# Connect to vector_db\n",
    "\\c vector_db\n",
    "# list tables\n",
    "\\dt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Create a Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-vector-stores-postgres\n",
      "  Downloading llama_index_vector_stores_postgres-0.1.14-py3-none-any.whl.metadata (853 bytes)\n",
      "Requirement already satisfied: asyncpg<0.30.0,>=0.29.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-vector-stores-postgres) (0.29.0)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.20 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-vector-stores-postgres) (0.10.67)\n",
      "Collecting pgvector<0.3.0,>=0.2.4 (from llama-index-vector-stores-postgres)\n",
      "  Downloading pgvector-0.2.5-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: psycopg2-binary<3.0.0,>=2.9.9 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-vector-stores-postgres) (2.9.9)\n",
      "Requirement already satisfied: sqlalchemy<2.1,>=1.4.49 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from sqlalchemy[asyncio]<2.1,>=1.4.49->llama-index-vector-stores-postgres) (2.0.32)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from asyncpg<0.30.0,>=0.29.0->llama-index-vector-stores-postgres) (4.0.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (2024.6.1)\n",
      "Requirement already satisfied: httpx in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (3.2.1)\n",
      "Requirement already satisfied: nltk!=3.9,>=3.8.1 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.41.1)\n",
      "Requirement already satisfied: pandas in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from sqlalchemy[asyncio]<2.1,>=1.4.49->llama-index-vector-stores-postgres) (3.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.9.4)\n",
      "Requirement already satisfied: click in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (2024.7.24)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.3.1)\n",
      "Requirement already satisfied: certifi in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (2.2.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (3.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.2.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.20->llama-index-vector-stores-postgres) (1.16.0)\n",
      "Downloading llama_index_vector_stores_postgres-0.1.14-py3-none-any.whl (8.4 kB)\n",
      "Downloading pgvector-0.2.5-py2.py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: pgvector, llama-index-vector-stores-postgres\n",
      "  Attempting uninstall: pgvector\n",
      "    Found existing installation: pgvector 0.3.2\n",
      "    Uninstalling pgvector-0.3.2:\n",
      "      Successfully uninstalled pgvector-0.3.2\n",
      "Successfully installed llama-index-vector-stores-postgres-0.1.14 pgvector-0.2.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-vector-stores-postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pgvector in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (0.2.5)\n",
      "Requirement already satisfied: numpy in /Users/david.amat/Library/Caches/pypoetry/virtualenvs/pdf-search-llm-rag-mykPbGiu-py3.9/lib/python3.9/site-packages (from pgvector) (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"llama2_paper\",\n",
    "    embed_dim=384,  # openai embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure PGVector locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "brew install pgvector\n",
    "\n",
    "# Go to PostgreSQL and run\n",
    "CREATE EXTENSION vector;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Nodes into Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['44783819-1990-45a2-8956-0e448558eb33',\n",
       " 'f912c161-b2bb-4f7f-8fee-8c8bc4eaeddc',\n",
       " '249b83ef-8dfa-48ed-b511-69cac739a5e2',\n",
       " '34d3139c-0770-4382-a69c-ed0506cab9dd',\n",
       " '482c3a7d-e4bf-4cea-bd41-ac49cdbeede4',\n",
       " 'f7442ce2-6a1a-4329-be44-cf7fc1537abc',\n",
       " '693af066-e688-4db8-954b-a428455bc51c',\n",
       " '183a15ee-2c88-4b23-979c-89a3a61e0f04',\n",
       " 'f36bc684-cbd4-4b7a-ae3d-c9fc257a1b4a',\n",
       " 'fe7ba88e-4472-4ab6-924e-f2fc89711975',\n",
       " 'b36b16a3-3a02-4afc-98f8-04f18cb3efce',\n",
       " 'ba0ae9a1-2a09-42cf-9708-4104afc7c59c',\n",
       " 'b86ca3f9-4e71-411d-9bed-8c358a245f98',\n",
       " '4c95b058-5951-4489-9bcd-3299fe0f864d',\n",
       " '484994df-598c-493d-80e6-918db55159fa',\n",
       " '4b030fe5-a752-4eb8-83f2-356b4e8b8633',\n",
       " 'b2c4837e-f13e-4185-b138-405bb67eccfd',\n",
       " 'b0c00856-688e-4811-9ecc-02b12c19804c',\n",
       " 'af94843d-c35a-4a0b-8b99-98e954458816',\n",
       " 'cbdd94fd-10fd-47e4-b9e7-e66593478460',\n",
       " 'c24e32d2-6c37-4a16-94d7-5968a62f2a4c',\n",
       " '8bffb065-b9bb-4725-923b-51278e60a4c0',\n",
       " 'f824f358-3636-4b0c-9971-df931cd8f43a',\n",
       " '1eb38f0a-7e0f-473f-ba54-65053caf1d22',\n",
       " 'ccc9f16f-7e07-48ff-99a0-d5ecaeffc670',\n",
       " '2dbdfc97-aecd-4ab4-9090-c36cc7992ba9',\n",
       " '06c6afa2-8d4e-4169-a8b2-91ff107ee8ea',\n",
       " 'a0dcf537-f62c-4ec9-beac-8b4446439a4c',\n",
       " 'a2084bfc-55e5-471c-a17f-9efe829247ce',\n",
       " '99065f74-28c1-410d-af21-63820de00465',\n",
       " '801bf6a4-affd-47db-b74d-011f70f97586',\n",
       " 'b4c5d929-1796-44fa-8db7-6ee5880d406d',\n",
       " '2da90b4b-449b-4747-be50-d80f887ece5b',\n",
       " 'bd7ba579-e6d8-4ed5-bf0c-11064d778832',\n",
       " 'c9cb9ce0-389a-4e13-ba49-8537777dd3ae',\n",
       " '159c8e2c-b280-4619-8692-98883a662842',\n",
       " '4eb278ed-dfdd-4d5f-8ffd-067684f79e34',\n",
       " 'bcc3b28d-e4a3-4934-b80a-15f8fa2b2ef0',\n",
       " '1fc4a1e0-53e2-4c6e-908d-046f8ecaaaad',\n",
       " '659e8e57-ad16-45d9-ac27-5479869aad92',\n",
       " '9ba27dc8-38a1-471f-b52c-329acdea43fa',\n",
       " '9b08c1bd-ecd5-420a-a66d-03f743d2bd7a',\n",
       " 'b2b81c9c-088e-4bfd-8d29-84f81913317d',\n",
       " '851383d7-56f4-46d9-bf3f-4ce63c6a9b9f',\n",
       " '66da2692-3835-4002-ad13-40f9239b64e8',\n",
       " 'de949638-c2ff-48e3-9553-2907929bce85',\n",
       " 'b6c36251-973c-4fad-b17f-6452567d0b28',\n",
       " 'eb6de0fb-e7af-415a-a3a8-e009d1950620',\n",
       " '23a97dbb-13ac-4350-919a-e0de212c7b0a',\n",
       " '94a549e3-fffd-4680-a9f0-c12fab4f17c9',\n",
       " 'f550abc3-a939-4186-93fd-6f3db8657b2d',\n",
       " '1f9270ba-1add-491f-aa5b-e9943dd3d70c',\n",
       " '54f65053-c259-4b92-845b-575b9e260636',\n",
       " 'c7327e66-8ca1-49f0-b8b7-cf2ddcce6ffb',\n",
       " '459de4fe-b333-4b81-9095-a370f1800b02',\n",
       " '46f028b2-b1a1-4640-b0f3-404390068da9',\n",
       " 'aed6a08a-1440-4b47-a0f2-fbe18375af09',\n",
       " 'd9f59a23-ec79-4ac5-a88e-26d20fae8e44',\n",
       " '9792fc6c-d164-4547-b481-3ecab0326d29',\n",
       " 'b6cfe8d0-b180-48ba-8745-ea874a104ffa',\n",
       " '4950282d-cb9d-4a64-9979-0af8af2b2c11',\n",
       " 'db32dc31-49d3-48de-acb4-adfc1ec45a86',\n",
       " '931cf4c8-29ca-491b-9c85-d63f9d22a291',\n",
       " '874e5f9a-d522-4cc4-becf-21cb4340d902',\n",
       " '11cc3698-bc0a-49dd-b7a4-a829fce38e65',\n",
       " '87d7ed93-0005-4820-9184-9a796fe79d93',\n",
       " 'e0bae684-8517-4f94-a93f-cd936727b9a8',\n",
       " 'f30b8bcd-03d2-48fa-98f8-b72fd617f166',\n",
       " 'a0211199-2ab1-4cf0-8ccf-8e16da864096',\n",
       " 'ee4d96b0-7212-4347-8ea3-442b5737d569',\n",
       " 'af299526-4206-4d4b-996e-405f9ffb4934',\n",
       " '44753104-c945-4f36-98e7-fca789ef2de8',\n",
       " '71a13b1b-a09e-4a99-b0cc-88d552533aba',\n",
       " 'f3ca5db2-e592-462f-b9b5-55d1f08b686b',\n",
       " '57692cda-d821-479c-8797-dead8ccb411c',\n",
       " 'f9e07fef-7f08-472e-809a-8e7681dd7e59',\n",
       " 'd0d2786e-dc16-44ab-95ec-12792f1fb12e',\n",
       " '4b0b0962-07b2-4bdb-b8a2-7116b1010cab',\n",
       " '0a241743-0301-4f64-8a00-f561e249f253',\n",
       " 'b5199ea5-91b1-418a-ab24-49a48eba0f84',\n",
       " 'cd43d793-916c-476f-a0e4-3093dc8816ac',\n",
       " 'fa382a57-9735-4f24-9b46-094756a36ec5',\n",
       " 'c35fbf00-0cdf-4ed9-88e4-93ae357d809c',\n",
       " '71b5b646-caea-477e-abe7-c410861f5312',\n",
       " 'e03199b9-1357-41c2-b5b0-9e43ad7c7551',\n",
       " 'cb0fd04e-e697-4bf3-aa00-85630eb0992e',\n",
       " 'c154e883-6aae-4e51-8f3e-a4ce81c76dfe',\n",
       " 'a7fef0e4-b6a2-4fdb-abfc-8dbca053424d',\n",
       " '532c6638-b12c-4565-a418-0174f7be0f2a',\n",
       " '9a87f7f6-1328-4f64-87a0-d2e3816c55c5',\n",
       " 'e427b21a-a0f6-499f-9c0a-12e129b59552',\n",
       " '96e473c3-c02d-4a33-a243-34cff5d06e0a',\n",
       " '16d762d1-164f-490f-bcb5-d8932e5b3a1c',\n",
       " '6de2af25-7d29-4394-85a7-c8f6e4b2630b',\n",
       " 'b36447f9-bc02-4b03-bccf-a41b37c83324',\n",
       " '90112896-4b98-4e3a-849d-0c8104b3d4b0',\n",
       " 'f9356293-1e61-4e79-a6a9-c6cd6127c898',\n",
       " '9628b42f-6ea8-4cd4-ab1e-ad5786b7a52c',\n",
       " '9a47d47c-ece4-4d5b-89cf-a2106ffd73a8',\n",
       " '83439d37-0e47-4098-92e6-3840cd83d156',\n",
       " 'c32cd8eb-e917-4568-944a-3e57e352a4b3',\n",
       " 'e253c98e-678f-4c29-b28f-4cf3ab82439e',\n",
       " 'ed925f2c-2128-4eab-84ac-4432f4b27a4b',\n",
       " '6d61f0b0-658e-4b15-a663-315761e83693',\n",
       " 'dbd67141-af87-4d5f-8752-240b68bf5a8e',\n",
       " '06756d9e-3f4e-4403-8c82-750f9407e984',\n",
       " '043ad3ef-a14f-47e8-9514-6abf5e1a844c']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Can you tell me about the key concepts for safety finetuning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate query embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = model.encode(query_str)\n",
    "query_embedding = list(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vector store query\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "query_mode = \"default\"\n",
    "# query_mode = \"sparse\"\n",
    "# query_mode = \"hybrid\"\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, \n",
    "    similarity_top_k=5, \n",
    "    mode=query_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run query\n",
    "query_result = vector_store.query(vector_store_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_result.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4\\nSafety\\nWARNING: this section contains examples of text that may be considered unsafe, offensive, o'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result.nodes[0].text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0\\n20\\n40\\n60\\n80\\n100\\nSafety Data Pct. (%)\\n0.01\\n0.02\\n0.03\\n0.04\\n0.05\\nFalse Refusal Rate (%)\\nHelpfulness\\n0'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result.nodes[1].text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'advice). The attack vectors explored consist of psychological manipulation (e.g., authority manipula'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result.nodes[2].text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum 0: 0.48233445251967333\n",
      "Sum 1: 0.4793740858139923\n",
      "Sum 2: 0.4716346697165157\n",
      "Sum 3: 0.46611180118375883\n",
      "Sum 4: 0.380216889394391\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(query_result.nodes)):\n",
    "    print(f\"Sum {i}: {query_result.similarities[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Results into a set of Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_result.nodes):\n",
    "    score: Optional[float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "    nodes_with_scores.append(NodeWithScore(node=node, score=score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        arr_query_embedding = self.embed_model(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        query_embedding = list(arr_query_embedding)\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorDBRetriever(\n",
    "    vector_store, model, query_mode=\"default\", similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the main LLM that will respond the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading url https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf to path /Users/david.amat/Library/Caches/llama_index/models/llama-2-13b-chat.Q4_0.gguf\n",
      "total size (MB): 7365.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7025it [04:29, 26.10it/s]                          \n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /Users/david.amat/Library/Caches/llama_index/models/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.34 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =   170.20 MiB, ( 1210.58 / 21845.34)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7023.90 MiB\n",
      "llm_load_tensors:      Metal buffer size =   170.20 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2973.75 MiB\n",
      "llama_kv_cache_init:      Metal KV buffer size =    76.25 MiB\n",
      "llama_new_context_with_model: KV self size  = 3050.00 MiB, K (f16): 1525.00 MiB, V (f16): 1525.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   352.63 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   352.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 627\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '40', 'llama.context_length': '4096', 'llama.attention.head_count': '40', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '13824', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "# model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\"\n",
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   16245.57 ms\n",
      "llama_print_timings:      sample time =       1.52 ms /    64 runs   (    0.02 ms per token, 42077.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46628.83 ms /  1766 tokens (   26.40 ms per token,    37.87 tokens per second)\n",
      "llama_print_timings:        eval time =    9433.39 ms /    63 runs   (  149.74 ms per token,     6.68 tokens per second)\n",
      "llama_print_timings:       total time =   56095.17 ms /  1829 tokens\n"
     ]
    }
   ],
   "source": [
    "query_str = \"How does Llama 2 perform compared to other open-source models?\"\n",
    "\n",
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llama 2 outperforms all open-source models, with results on par or better than PaLM (540B) on almost all benchmarks, except for coding benchmarks where there is a significant gap.\n",
      "\n",
      "Please let me know if you need any further information or clarification.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.source_nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr_llm_rag_v2",
   "language": "python",
   "name": "kr_llm_rag_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
